py_setup<source>
from pandas import read_csv
import pandas as pd
import rpy2.rinterface
from warnings import filterwarnings
from IPython.display import Image
from random import shuffle
filterwarnings('ignore', category=rpy2.rinterface.RRuntimeWarning)

# Load metadata file
mdf = read_csv('qiime_mapping_file.tsv', sep='\t')
mdf.set_index('#SampleID', inplace=True)

# Stores a list of values shared accross groups but unique within (for graphing)
group_ids = [] 
color_maps = {}
max_colors = []
# Create information for R color palettes
for group_name in mdf.columns:
    grouping = mdf[group_name]
    uni = grouping.nunique()
    group_ids.append(['color' + str(i) for i in range(uni)])
    # Get the category with the most colors to use for palette creation
    if uni > len(max_colors):
        max_colors = group_ids[-1]
    color_maps[group_name] = {x:'color' + str(list(set(grouping)).index(x)) for x in set(grouping)}
    
# Shuffle the colors so value near each other won't be similar shades
shuffle(max_colors, lambda: 0.25)

# Load the extention for jupyter
%load_ext rpy2.ipython
=====
r_setup<source>
%%R -i max_colors -o allColors
require(ggplot2)
require(RColorBrewer)
require(GGally)

# Create custom color palette
myColors <- brewer.pal(12, "Paired")
colorMaker <- colorRampPalette(myColors)
allColors <- colorMaker(length(unique(max_colors)))
# Rename the colors to match with the groups
names(allColors) <- max_colors
colScale <- scale_color_manual(name = ~GroupID, values = allColors)
colFill <- scale_fill_manual(name = ~GroupID, values = allColors)
=====
otu_py<source>
df = read_csv('otu_table.tsv', skiprows=1, header=0, sep='\t')
df.set_index('taxonomy', inplace=True)
df.iloc[:5, :5]
=====
taxa_py<source>
df = read_csv('{file1}', skiprows=1, sep='\t').melt(id_vars='#OTU ID')
=====
taxa_r<source>
%%R -i df
p <- ggplot(df, aes(x = variable, y = value, fill = X.OTU.ID)) +
     geom_bar(stat = "identity") +
     ggtitle("Taxa Summary") +
     labs(x = "Sample ID") +
     theme(text = element_text(size = 8.5,
                               face = "bold"),
           element_line(size = 0.1)) +
     scale_y_discrete(name = "OTU Percentage",
                      limits = c(0, 1),
                      expand = c(0, 0)) +
     theme(legend.text=element_text(size=7),
           legend.key.size = unit(0.1, "in"),
           legend.position = "bottom",
           legend.direction="vertical",
           plot.title = element_text(hjust = 0.5))

ggsave("{plot}", height = 6, width = 8)
=====
legend_py<source>
# Create the list for the legend
color_groups = []
# Create a mapping from the color names to the identifying strings
colors = {{list(allColors.names)[x]:list(allColors)[x] for x in range(len(allColors))}}
for group_name in mdf.columns:
    group = mdf[group_name]
    # Only show colors for catagories that will be graphed
    if group.nunique() > 1 and group.nunique() < len(group) and 'Date' not in group_name:
        color_names = [colors[color_maps[group_name][x]] for x in group]
        # Create a dataframe with the colors and grouping
        group = pd.DataFrame({{'Grouping': group, 'Color': color_names}})
        # Add the name of each group for the title
        group = group.assign(GroupName=[group_name for x in range(len(group))])
        # Drop duplicates and add to the list
        color_groups.append(group.drop_duplicates('Color'))
=====
legend_r<source>
%%R -i color_groups
png('{plot}', width = 6, height = 6, unit='in', res=200)
attach(mtcars)
layout(mat=matrix(c(1:9), 3, 3, byrow = TRUE))
par(mar=c(0.0,0.0,0.0,0.0))
for (i in c(1:length(color_groups))) {{
    plot(NULL, xaxt='n',yaxt='n',bty='n',ylab='n',xlab='n', xlim=0:1, ylim=0:1)
    legend("top",
           legend = color_groups[[i]]$Grouping,
           col = color_groups[[i]]$Color,
           title = color_groups[[i]]$GroupName[[1]],
           pch=16, pt.cex=2, cex=1.5)
}}
dev.off()
=====
alpha_py_qiime1<source>
# Read in the data
df = read_csv('{file1}', sep='\t')

# Drop unused columns
df.drop('Unnamed: 0', axis=1, inplace=True)
df.drop('iteration', axis=1, inplace=True)

# Set new indeces
df.set_index('sequences per sample', inplace=True)

# Create groupings based on metadata values
group_means = []

for group_name in mdf.columns:
    grouping = mdf[group_name]
    # Only create groups for values that aren't all the same or all unique
    if grouping.nunique() > 1 and grouping.nunique() < len(grouping) and 'Date' not in group_name:
        # Calculate the means accross iterations
        groups = df.groupby('sequences per sample')
        means = groups.mean()
        
        # Join the data and metadata (requires transposing the data)
        joined_means = means.T.join(grouping, how='outer')
        
        # Group by metadata value and calculate the mean
        grouped_means = joined_means.groupby(group_name)

        # Traspose the data frame again and set the indeces to be a seperate column
        group = grouped_means.mean().T.reset_index(level=0).melt(id_vars='index')
        error = grouped_means.sem().T.reset_index(level=0).melt(id_vars='index')
        
        # Assign the error values
        group = group.assign(Error=error['value'])
        
        # Assign information for the colors
        colors = [color_maps[group_name][x] for x in group[group_name]]
        group = group.assign(GroupID=colors)
        
        # Assign information for groups
        group_names = [group_name for x in group[group_name]]
        group = group.assign(GroupName=group_names)
        
        # Rename some columns
        new_names = {{
            'index':'SequencesPerSample',
            group_name: 'Grouping',
            'value': 'AverageValue'
        }}
        group = group.rename(index=str, columns=new_names)
        group_means.append(group)
        
# Stack all the different groups into a single dataframe
df = pd.concat(group_means, axis=0, sort=False)
=====
alpha_py_qiime2<source>
# Read in the data
df = read_csv({file1}, sep=',')

# Reshape the data into (mostly) long format
headers = list(set(mdf.columns).intersection(set(df.columns)))
df = df.melt(id_vars=['sample-id'] + headers)

# Remove info on specific iterations to allow for grouping by value
replacements = {{x:int(x.split('_')[0].split('-')[1]) for x in df.variable.unique()}}
df.variable.replace(replacements, inplace=True)

# For storing 
group_means = []

for group_name in headers:
    # Only create groups for values that aren't all the same or all unique
    if (mdf[group_name].nunique() > 1 and
        mdf[group_name].nunique() < len(mdf[group_name]) and
        'Date' not in group_name):
        
        # Remove the metadata not relevant to this grouping
        groups = df[['sample-id', 'variable', 'value', group_name]]
        
        # Calculate the means accross iterations
        agger = {{'value': 'mean', group_name: 'first'}}
        groups = groups.groupby(['sample-id', 'variable']).agg(agger).reset_index()
        
        # Add a column to store the errors
        groups = groups.assign(Error=groups.value)
        
        # Group by metadata value and calculate the means and error
        agger = {{'Error': 'sem', 'value': 'mean'}}
        group = groups.groupby([group_name, 'variable']).agg(agger).reset_index()
        
        # Assign information for the colors
        colors = [color_maps[group_name][x] for x in group[group_name]]
        group = group.assign(GroupID=colors)
        
        # Assign information for grouping, drop specific catagories
        group_names = [group_name for x in group[group_name]]
        group = group.assign(GroupName=group_names).drop(group_name, axis='columns')
        
        # Rename columns and append to the list of dataframes
        new_names = {{
            'variable': 'SamplingDepth',
            'value': 'AverageValue',
             group_name: 'Grouping'
        }}
        group_means.append(group.rename(index=str, columns=new_names))
        
# Stack all the different groups into a single dataframe
df = pd.concat(group_means, axis=0, sort=False)
df.SamplingDepth = df.SamplingDepth.astype(int)
=====
alpha_r<source>
%%R -i df
pd <- position_dodge(width = 50)

p <- ggplot(data = df, aes(x = {xaxis}, y = AverageValue, color = GroupID)) +
     geom_errorbar(aes(ymin=AverageValue-Error, ymax=AverageValue+Error), width=100, position = pd) +
     geom_point(stat='identity', position = pd, size = 1) +
     geom_line(stat='identity', position = pd) +
     facet_wrap(~GroupName) + colFill + colScale +
     theme(legend.position = 'none')

# Save plots
ggsave('{file1}', height = 6, width = 6)
=====
beta_py<source>
import pandas as pd
with open('{file1}') as f:
    page = f.read()
    
store = {{}}
# Parse the PCA information file
for i, line in enumerate(page.split('\n')):
    parts = line.split('\t')
    if i == 0:
        length = int(parts[1])
    if i > 9 :
        if line == '':
            break
        store[parts[0]] = list(map(float, parts[1:length]))
        
# Create a dataframe and name the axes
df = pd.DataFrame.from_dict(store).T
cols = {{x:'PC{{}}'.format(x + 1) for x in df.columns}}
df = df.rename(index=str, columns=cols)

# Assign GroupIDs based on metadata
col = 'SpecimenBodySite'
samples = mdf[col][df.axes[0]]
df = df.assign(GroupID=[color_maps[col][x] for x in samples])
=====
beta_r<source>
%%R -i df
png('{file1}', width = 6, height = 6, unit='in', res=200)
p <- ggpairs(df[,c(1:3)], aes(color = df$GroupID))

# Add the color palette to each of the plots
for(i in 1:p$nrow) {{
    for(j in 1:p$ncol){{
        p[i,j] <- p[i,j] + colScale + colFill
    }}
}}
print(p)
dev.off()
